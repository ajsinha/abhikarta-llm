{
  "template_id": "wf_semantic_router",
  "name": "Semantic Router Workflow",
  "description": "Intelligently routes requests based on semantic understanding of intent. Uses embedding-like similarity matching to direct inputs to specialized handlers.",
  "category": "intermediate",
  "icon": "bi-signpost-2-fill",
  "difficulty": "intermediate",
  "tags": [
    "routing",
    "semantic",
    "intent",
    "classification",
    "dynamic"
  ],
  "workflow": {
    "routing_config": {
      "confidence_threshold": 0.7,
      "fallback_enabled": true,
      "multi_intent_support": true
    },
    "nodes": [
      {
        "id": "intent_analyzer",
        "type": "llm",
        "name": "Intent Analyzer",
        "description": "Analyze input to determine intent",
        "position": {
          "x": 100,
          "y": 300
        },
        "config": {
          "prompt_template": "Analyze the intent of this input:\n\n\"{input}\"\n\nClassify into one or more intents:\n\nPRIMARY INTENTS:\n1. INFORMATION_SEEKING: Wants to learn or understand something\n2. TASK_EXECUTION: Wants something done/created\n3. ANALYSIS_REQUEST: Wants data/situation analyzed\n4. CREATIVE_REQUEST: Wants creative content generated\n5. PROBLEM_SOLVING: Has a problem needing solution\n6. COMPARISON: Wants options compared\n7. RECOMMENDATION: Wants suggestions/advice\n8. CLARIFICATION: Wants something explained\n9. VALIDATION: Wants something checked/verified\n10. CONVERSATION: General chat/greeting\n\nProvide:\n- PRIMARY_INTENT: Most likely intent\n- SECONDARY_INTENT: Second most likely (if applicable)\n- CONFIDENCE: 0-100% for primary intent\n- KEY_ENTITIES: Important elements in the request\n- COMPLEXITY: simple/moderate/complex",
          "output_key": "intent_analysis",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "context_enricher",
        "type": "llm",
        "name": "Context Enricher",
        "description": "Enrich with additional context",
        "position": {
          "x": 300,
          "y": 300
        },
        "config": {
          "prompt_template": "Enrich this request with context:\n\nINPUT: {input}\nINTENT ANALYSIS: {intent_analysis}\n\nAdd context:\n1. DOMAIN: What field/area does this relate to?\n2. EXPERTISE_NEEDED: What knowledge is required?\n3. OUTPUT_FORMAT: What format is expected?\n4. CONSTRAINTS: Any implicit constraints?\n5. URGENCY: Implied urgency level\n6. DEPTH_NEEDED: Surface-level or deep dive?",
          "output_key": "enriched_context",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "semantic_router",
        "type": "condition",
        "name": "Semantic Router",
        "description": "Route to specialized handler",
        "position": {
          "x": 500,
          "y": 300
        },
        "config": {
          "conditions": [
            {
              "condition": "INFORMATION_SEEKING in intent_analysis",
              "target": "information_handler"
            },
            {
              "condition": "TASK_EXECUTION in intent_analysis",
              "target": "task_handler"
            },
            {
              "condition": "ANALYSIS_REQUEST in intent_analysis",
              "target": "analysis_handler"
            },
            {
              "condition": "CREATIVE_REQUEST in intent_analysis",
              "target": "creative_handler"
            },
            {
              "condition": "PROBLEM_SOLVING in intent_analysis",
              "target": "problem_solver"
            },
            {
              "condition": "COMPARISON in intent_analysis",
              "target": "comparison_handler"
            },
            {
              "condition": "RECOMMENDATION in intent_analysis",
              "target": "recommendation_handler"
            },
            {
              "default": true,
              "target": "general_handler"
            }
          ]
        }
      },
      {
        "id": "information_handler",
        "type": "llm",
        "name": "Information Handler",
        "description": "Handle information-seeking requests",
        "position": {
          "x": 750,
          "y": 50
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Information Seeking\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nProvide comprehensive information:\n1. Direct answer to the question\n2. Key facts and details\n3. Context and background\n4. Related topics to explore\n5. Sources or references if applicable\n\nBe thorough yet accessible.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "task_handler",
        "type": "llm",
        "name": "Task Handler",
        "description": "Handle task execution requests",
        "position": {
          "x": 750,
          "y": 150
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Task Execution\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nExecute the task:\n1. Understand the specific deliverable needed\n2. Create the requested output\n3. Ensure quality and completeness\n4. Provide any necessary instructions\n\nDeliver the completed task.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "analysis_handler",
        "type": "llm",
        "name": "Analysis Handler",
        "description": "Handle analysis requests",
        "position": {
          "x": 750,
          "y": 250
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Analysis\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nProvide analysis:\n1. Break down the subject\n2. Identify patterns and insights\n3. Present findings clearly\n4. Draw conclusions\n5. Provide recommendations based on analysis\n\nBe analytical and data-driven.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "creative_handler",
        "type": "llm",
        "name": "Creative Handler",
        "description": "Handle creative requests",
        "position": {
          "x": 750,
          "y": 350
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Creative Content\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nCreate creative content:\n1. Understand the creative vision\n2. Generate original content\n3. Ensure engagement and quality\n4. Match the desired tone/style\n5. Offer variations if helpful\n\nBe creative and original.",
          "output_key": "handler_output",
          "temperature": 0.9,
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434"
        }
      },
      {
        "id": "problem_solver",
        "type": "llm",
        "name": "Problem Solver",
        "description": "Handle problem-solving requests",
        "position": {
          "x": 750,
          "y": 450
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Problem Solving\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nSolve the problem:\n1. Clearly define the problem\n2. Analyze root causes\n3. Generate potential solutions\n4. Evaluate trade-offs\n5. Recommend best approach\n6. Provide implementation steps\n\nBe systematic and solution-focused.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "comparison_handler",
        "type": "llm",
        "name": "Comparison Handler",
        "description": "Handle comparison requests",
        "position": {
          "x": 750,
          "y": 550
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Comparison\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nProvide comparison:\n1. Identify items being compared\n2. Define comparison criteria\n3. Analyze each option fairly\n4. Highlight key differences\n5. Provide balanced assessment\n6. Suggest best fit based on context\n\nBe objective and balanced.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "recommendation_handler",
        "type": "llm",
        "name": "Recommendation Handler",
        "description": "Handle recommendation requests",
        "position": {
          "x": 750,
          "y": 650
        },
        "config": {
          "prompt_template": "SPECIALIZED HANDLER: Recommendations\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nProvide recommendations:\n1. Understand needs and preferences\n2. Consider constraints\n3. Offer ranked recommendations\n4. Explain rationale for each\n5. Highlight trade-offs\n6. Suggest how to decide\n\nBe helpful and personalized.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "general_handler",
        "type": "llm",
        "name": "General Handler",
        "description": "Handle unclassified requests",
        "position": {
          "x": 750,
          "y": 750
        },
        "config": {
          "prompt_template": "GENERAL HANDLER\n\nREQUEST: {input}\nCONTEXT: {enriched_context}\n\nRespond helpfully:\n1. Address the request directly\n2. Provide useful information\n3. Ask clarifying questions if needed\n4. Suggest how to help further\n\nBe friendly and helpful.",
          "output_key": "handler_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "quality_checker",
        "type": "llm",
        "name": "Quality Checker",
        "description": "Check response quality",
        "position": {
          "x": 950,
          "y": 300
        },
        "config": {
          "prompt_template": "Check response quality:\n\nORIGINAL REQUEST: {input}\nINTENT: {intent_analysis}\nRESPONSE: {handler_output}\n\nVerify:\n1. Does response match the intent? (Yes/No)\n2. Is response complete? (Yes/Partial/No)\n3. Quality score (1-10)\n4. Any gaps or issues?\n\nIf quality < 7, note what's missing.",
          "output_key": "quality_check",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      },
      {
        "id": "response_enhancer",
        "type": "llm",
        "name": "Response Enhancer",
        "description": "Enhance response if needed",
        "position": {
          "x": 1100,
          "y": 300
        },
        "config": {
          "prompt_template": "Enhance this response:\n\nORIGINAL REQUEST: {input}\nCURRENT RESPONSE: {handler_output}\nQUALITY CHECK: {quality_check}\n\nEnhance by:\n1. Filling any gaps noted\n2. Improving clarity\n3. Adding helpful context\n4. Ensuring completeness\n\nProvide the enhanced response.",
          "output_key": "final_output",
          "provider": "ollama",
          "model": "llama3.2:3b",
          "base_url": "http://localhost:11434",
          "temperature": 0.6
        }
      }
    ],
    "edges": [
      {
        "source": "intent_analyzer",
        "target": "context_enricher"
      },
      {
        "source": "context_enricher",
        "target": "semantic_router"
      },
      {
        "source": "semantic_router",
        "target": "information_handler",
        "condition": "INFORMATION"
      },
      {
        "source": "semantic_router",
        "target": "task_handler",
        "condition": "TASK"
      },
      {
        "source": "semantic_router",
        "target": "analysis_handler",
        "condition": "ANALYSIS"
      },
      {
        "source": "semantic_router",
        "target": "creative_handler",
        "condition": "CREATIVE"
      },
      {
        "source": "semantic_router",
        "target": "problem_solver",
        "condition": "PROBLEM"
      },
      {
        "source": "semantic_router",
        "target": "comparison_handler",
        "condition": "COMPARISON"
      },
      {
        "source": "semantic_router",
        "target": "recommendation_handler",
        "condition": "RECOMMENDATION"
      },
      {
        "source": "semantic_router",
        "target": "general_handler",
        "condition": "default"
      },
      {
        "source": "information_handler",
        "target": "quality_checker"
      },
      {
        "source": "task_handler",
        "target": "quality_checker"
      },
      {
        "source": "analysis_handler",
        "target": "quality_checker"
      },
      {
        "source": "creative_handler",
        "target": "quality_checker"
      },
      {
        "source": "problem_solver",
        "target": "quality_checker"
      },
      {
        "source": "comparison_handler",
        "target": "quality_checker"
      },
      {
        "source": "recommendation_handler",
        "target": "quality_checker"
      },
      {
        "source": "general_handler",
        "target": "quality_checker"
      },
      {
        "source": "quality_checker",
        "target": "response_enhancer"
      }
    ],
    "entry_point": "intent_analyzer",
    "output_node": "response_enhancer"
  },
  "llm_config": {
    "provider": "ollama",
    "model": "llama3.2:3b",
    "base_url": "http://localhost:11434",
    "temperature": 0.6
  },
  "sample_inputs": [
    {
      "query": "What are the main differences between REST and GraphQL APIs?",
      "input": "What are the main differences between REST and GraphQL APIs?"
    },
    {
      "query": "Write a haiku about programming",
      "input": "Write a haiku about programming"
    },
    {
      "query": "I'm trying to decide between Python and JavaScript for my web project",
      "input": "I'm trying to decide between Python and JavaScript for my web project"
    },
    {
      "query": "Analyze the pros and cons of remote work",
      "input": "Analyze the pros and cons of remote work"
    },
    {
      "query": "My code keeps crashing with a memory error - help!",
      "input": "My code keeps crashing with a memory error - help!"
    }
  ]
}