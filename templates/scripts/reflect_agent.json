{
    "template_id": "script_reflect_agent",
    "name": "Self-Reflecting Quality Agent",
    "description": "An agent that iteratively refines output through self-reflection",
    "entity_type": "agent",
    "agent_type": "reflect",
    "category": "quality",
    "difficulty": "advanced",
    "icon": "bi-arrow-repeat",
    "tags": [
        "agent",
        "reflect",
        "quality",
        "refinement"
    ],
    "use_count": 0,
    "script_content": "\"\"\"\nSelf-Reflecting Quality Agent - Python Script Mode\n===================================================\nIteratively improves output through quality evaluation.\n\"\"\"\n\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom abc import ABC, abstractmethod\n\n\nclass QualityDimension(Enum):\n    ACCURACY = \"accuracy\"\n    COMPLETENESS = \"completeness\"\n    CLARITY = \"clarity\"\n\n\nclass ReflectionPhase(Enum):\n    GENERATE = \"generate\"\n    EVALUATE = \"evaluate\"\n    REFINE = \"refine\"\n\n\n@dataclass\nclass QualityScore:\n    dimension: QualityDimension\n    score: float\n    feedback: str\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"dimension\": self.dimension.value,\n            \"score\": self.score,\n            \"feedback\": self.feedback\n        }\n\n\n@dataclass\nclass Critique:\n    strengths: List[str] = field(default_factory=list)\n    weaknesses: List[str] = field(default_factory=list)\n    suggestions: List[str] = field(default_factory=list)\n    overall_score: float = 0.0\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"strengths\": self.strengths,\n            \"weaknesses\": self.weaknesses,\n            \"suggestions\": self.suggestions,\n            \"overall_score\": self.overall_score\n        }\n\n\nclass QualityEvaluator(ABC):\n    @property\n    @abstractmethod\n    def dimension(self) -> QualityDimension:\n        pass\n    \n    @abstractmethod\n    def evaluate(self, content: str, context: Dict) -> QualityScore:\n        pass\n\n\nclass AccuracyEvaluator(QualityEvaluator):\n    @property\n    def dimension(self) -> QualityDimension:\n        return QualityDimension.ACCURACY\n    \n    def evaluate(self, content: str, context: Dict) -> QualityScore:\n        score = 0.8 if len(content) > 50 else 0.5\n        return QualityScore(self.dimension, score, \"Accuracy check\")\n\n\nclass CompletenessEvaluator(QualityEvaluator):\n    @property\n    def dimension(self) -> QualityDimension:\n        return QualityDimension.COMPLETENESS\n    \n    def evaluate(self, content: str, context: Dict) -> QualityScore:\n        topics = context.get(\"expected_topics\", [])\n        found = sum(1 for t in topics if t.lower() in content.lower())\n        score = found / max(len(topics), 1)\n        return QualityScore(self.dimension, score, f\"{found}/{len(topics)} topics\")\n\n\nclass ClarityEvaluator(QualityEvaluator):\n    @property\n    def dimension(self) -> QualityDimension:\n        return QualityDimension.CLARITY\n    \n    def evaluate(self, content: str, context: Dict) -> QualityScore:\n        words = content.split()\n        avg_len = sum(len(w) for w in words) / max(len(words), 1)\n        score = max(0, min(1, 1.0 - abs(avg_len - 5) / 10))\n        return QualityScore(self.dimension, score, f\"Avg word len: {avg_len:.1f}\")\n\n\nclass ReflectionEngine:\n    def __init__(self):\n        self.evaluators = [\n            AccuracyEvaluator(),\n            CompletenessEvaluator(),\n            ClarityEvaluator()\n        ]\n        self.quality_threshold = 0.75\n        self.max_iterations = 5\n    \n    def evaluate(self, content: str, context: Dict) -> List[QualityScore]:\n        return [e.evaluate(content, context) for e in self.evaluators]\n    \n    def generate_critique(self, scores: List[QualityScore]) -> Critique:\n        critique = Critique()\n        for score in scores:\n            if score.score >= 0.8:\n                critique.strengths.append(f\"Good {score.dimension.value}\")\n            elif score.score < 0.5:\n                critique.weaknesses.append(f\"Weak {score.dimension.value}\")\n                critique.suggestions.append(f\"Improve {score.dimension.value}\")\n        critique.overall_score = sum(s.score for s in scores) / len(scores)\n        return critique\n    \n    def refine(self, content: str, critique: Critique) -> str:\n        if \"completeness\" in str(critique.suggestions).lower():\n            content += \" [Additional details added]\"\n        return content\n    \n    def run_cycle(self, prompt: str, content: str = \"\") -> Dict[str, Any]:\n        content = content or f\"Response to: {prompt}\"\n        context = {\"expected_topics\": prompt.split()[:3]}\n        \n        for i in range(self.max_iterations):\n            scores = self.evaluate(content, context)\n            critique = self.generate_critique(scores)\n            \n            if critique.overall_score >= self.quality_threshold:\n                break\n            content = self.refine(content, critique)\n        \n        return {\n            \"final_content\": content,\n            \"iterations\": i + 1,\n            \"final_score\": critique.overall_score\n        }\n\n\nSYSTEM_PROMPT = \"\"\"You are a self-reflecting agent.\n1. Generate response\n2. Evaluate quality\n3. Identify weaknesses\n4. Refine iteratively\n5. Deliver quality output\n\"\"\"\n\nengine = ReflectionEngine()\n\nagent = {\n    \"name\": \"Self-Reflecting Quality Agent\",\n    \"description\": \"Improves output through reflection\",\n    \"agent_type\": \"reflect\",\n    \"version\": \"1.0.0\",\n    \"system_prompt\": SYSTEM_PROMPT,\n    \"tools\": [\"evaluate\", \"critique\", \"refine\"],\n    \"llm_config\": {\n        \"provider\": \"ollama\",\n        \"model\": \"llama3.2:3b\",\n        \"temperature\": 0.5\n    },\n    \"reflection_config\": {\n        \"max_iterations\": 5,\n        \"quality_threshold\": 0.75\n    },\n    \"tags\": [\"quality\", \"reflect\", \"refinement\", \"python-script\"],\n    \"category\": \"quality\"\n}\n\n__export__ = agent\n\n\ndef execute(input_data: Dict[str, Any]) -> Dict[str, Any]:\n    prompt = input_data.get(\"prompt\", \"\")\n    content = input_data.get(\"content\", \"\")\n    result = engine.run_cycle(prompt, content)\n    \n    return {\n        \"success\": True,\n        \"response\": result[\"final_content\"],\n        \"iterations\": result[\"iterations\"],\n        \"score\": result[\"final_score\"]\n    }\n\n\ndef validate() -> tuple:\n    if not agent.get(\"reflection_config\"):\n        return False, \"Reflection config required\"\n    return True, \"Reflect Agent configuration is valid\"\n"
}